{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import json\n",
    "from fastai2.basics import *\n",
    "from fastai2.text.all import *\n",
    "from fastai2.vision.gan import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, out_size: int):\n",
    "        super().__init__()\n",
    "        self.awd_lstm = AWD_LSTM(\n",
    "            vocab_size, \n",
    "            emb_sz=400, n_hid=1152, n_layers=3, pad_token=1, bidir=False, hidden_p=0.15, input_p=0.25, embed_p=0.02, weight_p=0.2,\n",
    "        )\n",
    "        self.hid_proj = nn.Linear(self.awd_lstm.emb_sz, out_size)\n",
    "\n",
    "    def forward(self, inp_ids):\n",
    "        \"inp_ids: (bs, seq_len), summary: (bs, out_size))\"\n",
    "        self.reset()\n",
    "        awd_lstm_out = self.awd_lstm(inp_ids) # awd_lstm_out: (bs, seq_len, emb_sz=400)\n",
    "        awd_lstm_hid = self.awd_lstm.hidden \n",
    "        # hid[0], hid[1]: ( (1, bs, 1152), (1, bs, 1152) )\n",
    "        # hid[2]: ( (1, bs, 400), (1, bs, 400) )\n",
    "        \n",
    "#         summary = awd_lstm_hid[2][0].squeeze(dim=0) # (bs, 400)  \n",
    "        summary = awd_lstm_out.mean(dim=1) # (bs, 400)\n",
    "        summary = self.hid_proj(summary) # (bs, out_size)\n",
    "\n",
    "        return summary\n",
    "    def reset(self):\n",
    "        self.awd_lstm.reset()\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, awd_lstm_path: Path, vocab_path: Path, \n",
    "                        out_size):\n",
    "        vocab = json.loads(vocab_path.read_text())\n",
    "        ret = cls(len(vocab), out_size)\n",
    "        ret.awd_lstm.load_state_dict(torch.load(awd_lstm_path))\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size=8472, out_size=100)\n",
    "inp_ids = torch.randint(0, 100, (16, 20))\n",
    "summary = encoder(inp_ids)\n",
    "test_eq(summary.shape, (16, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "encoder = Encoder.from_pretrained(Path('./coco_small/awd_lstm-1.pt'), Path('./coco_small/vocab.json'), 100)\n",
    "inp_ids = torch.randint(0, 100, (16, 20))\n",
    "summary = encoder(inp_ids)\n",
    "test_eq(summary.shape, (16, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_size: int, inp_size: int, num_layers=3):\n",
    "        super().__init__()\n",
    "        store_attr(self, 'out_size,inp_size,num_layers')\n",
    "        layers = self.gen_layers()\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    def gen_layers(self):\n",
    "        up_sample = nn.Upsample(self.out_size//2**self.num_layers)\n",
    "        num_c = 64*2**self.num_layers\n",
    "        first_conv = ConvLayer(self.inp_size, num_c, 3, 1)\n",
    "        other_conv = [ConvLayer(num_c//2**i, num_c//2**(i+1), 4, 2, 1, transpose=True) for i in range(self.num_layers)]\n",
    "        last_conv = nn.Conv2d(64, 3, 3, 1, 1, bias=False)\n",
    "        last_act = nn.Tanh()\n",
    "        return [AddChannels(2), up_sample, first_conv] + other_conv + [last_conv, last_act]\n",
    "    def change_out_size(self, out_size: int):\n",
    "        self.out_size = out_size\n",
    "        self.layers[1] = nn.Upsample(out_size//2**self.num_layers)\n",
    "    def forward(self, enc_summary):\n",
    "        \"enc_summary: (bs, inp_size), out: (bs, 3, out_size, out_size)\"\n",
    "        return self.layers(enc_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder (Input shape: ['16 x 100'])\n",
       "================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "================================================================\n",
       "AddChannels          16 x 100 x 1 x 1     0          False     \n",
       "________________________________________________________________\n",
       "Upsample             16 x 100 x 8 x 8     0          False     \n",
       "________________________________________________________________\n",
       "Conv2d               16 x 512 x 8 x 8     460,800    True      \n",
       "________________________________________________________________\n",
       "BatchNorm2d          16 x 512 x 8 x 8     1,024      True      \n",
       "________________________________________________________________\n",
       "ReLU                 16 x 512 x 8 x 8     0          False     \n",
       "________________________________________________________________\n",
       "ConvTranspose2d      16 x 256 x 16 x 16   2,097,152  True      \n",
       "________________________________________________________________\n",
       "BatchNorm2d          16 x 256 x 16 x 16   512        True      \n",
       "________________________________________________________________\n",
       "ReLU                 16 x 256 x 16 x 16   0          False     \n",
       "________________________________________________________________\n",
       "ConvTranspose2d      16 x 128 x 32 x 32   524,288    True      \n",
       "________________________________________________________________\n",
       "BatchNorm2d          16 x 128 x 32 x 32   256        True      \n",
       "________________________________________________________________\n",
       "ReLU                 16 x 128 x 32 x 32   0          False     \n",
       "________________________________________________________________\n",
       "ConvTranspose2d      16 x 64 x 64 x 64    131,072    True      \n",
       "________________________________________________________________\n",
       "BatchNorm2d          16 x 64 x 64 x 64    128        True      \n",
       "________________________________________________________________\n",
       "ReLU                 16 x 64 x 64 x 64    0          False     \n",
       "________________________________________________________________\n",
       "Conv2d               16 x 3 x 64 x 64     1,728      True      \n",
       "________________________________________________________________\n",
       "Tanh                 16 x 3 x 64 x 64     0          False     \n",
       "________________________________________________________________\n",
       "\n",
       "Total params: 3,216,960\n",
       "Total trainable params: 3,216,960\n",
       "Total non-trainable params: 0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(64, 100)\n",
    "enc_summary = torch.randn(16, 100)\n",
    "out = decoder(enc_summary)\n",
    "test_eq(out.shape, (16, 3, 64, 64))\n",
    "decoder.summary(enc_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder (Input shape: ['16 x 100'])\n",
       "================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "================================================================\n",
       "AddChannels          16 x 100 x 1 x 1     0          False     \n",
       "________________________________________________________________\n",
       "Upsample             16 x 100 x 12 x 12   0          False     \n",
       "________________________________________________________________\n",
       "Conv2d               16 x 512 x 12 x 12   460,800    True      \n",
       "________________________________________________________________\n",
       "BatchNorm2d          16 x 512 x 12 x 12   1,024      True      \n",
       "________________________________________________________________\n",
       "ReLU                 16 x 512 x 12 x 12   0          False     \n",
       "________________________________________________________________\n",
       "ConvTranspose2d      16 x 256 x 24 x 24   2,097,152  True      \n",
       "________________________________________________________________\n",
       "BatchNorm2d          16 x 256 x 24 x 24   512        True      \n",
       "________________________________________________________________\n",
       "ReLU                 16 x 256 x 24 x 24   0          False     \n",
       "________________________________________________________________\n",
       "ConvTranspose2d      16 x 128 x 48 x 48   524,288    True      \n",
       "________________________________________________________________\n",
       "BatchNorm2d          16 x 128 x 48 x 48   256        True      \n",
       "________________________________________________________________\n",
       "ReLU                 16 x 128 x 48 x 48   0          False     \n",
       "________________________________________________________________\n",
       "ConvTranspose2d      16 x 64 x 96 x 96    131,072    True      \n",
       "________________________________________________________________\n",
       "BatchNorm2d          16 x 64 x 96 x 96    128        True      \n",
       "________________________________________________________________\n",
       "ReLU                 16 x 64 x 96 x 96    0          False     \n",
       "________________________________________________________________\n",
       "Conv2d               16 x 3 x 96 x 96     1,728      True      \n",
       "________________________________________________________________\n",
       "Tanh                 16 x 3 x 96 x 96     0          False     \n",
       "________________________________________________________________\n",
       "\n",
       "Total params: 3,216,960\n",
       "Total trainable params: 3,216,960\n",
       "Total non-trainable params: 0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder2 = Decoder(96, 100)\n",
    "decoder2.load_state_dict(decoder.state_dict())\n",
    "enc_summary = torch.randn(16, 100)\n",
    "out = decoder2(enc_summary)\n",
    "test_eq(out.shape, (16, 3, 96, 96))\n",
    "decoder2.summary(enc_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder (Input shape: ['16 x 100'])\n",
       "================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "================================================================\n",
       "AddChannels          16 x 100 x 1 x 1     0          False     \n",
       "________________________________________________________________\n",
       "Upsample             16 x 100 x 16 x 16   0          False     \n",
       "________________________________________________________________\n",
       "Conv2d               16 x 512 x 16 x 16   460,800    True      \n",
       "________________________________________________________________\n",
       "BatchNorm2d          16 x 512 x 16 x 16   1,024      True      \n",
       "________________________________________________________________\n",
       "ReLU                 16 x 512 x 16 x 16   0          False     \n",
       "________________________________________________________________\n",
       "ConvTranspose2d      16 x 256 x 32 x 32   2,097,152  True      \n",
       "________________________________________________________________\n",
       "BatchNorm2d          16 x 256 x 32 x 32   512        True      \n",
       "________________________________________________________________\n",
       "ReLU                 16 x 256 x 32 x 32   0          False     \n",
       "________________________________________________________________\n",
       "ConvTranspose2d      16 x 128 x 64 x 64   524,288    True      \n",
       "________________________________________________________________\n",
       "BatchNorm2d          16 x 128 x 64 x 64   256        True      \n",
       "________________________________________________________________\n",
       "ReLU                 16 x 128 x 64 x 64   0          False     \n",
       "________________________________________________________________\n",
       "ConvTranspose2d      16 x 64 x 128 x 128  131,072    True      \n",
       "________________________________________________________________\n",
       "BatchNorm2d          16 x 64 x 128 x 128  128        True      \n",
       "________________________________________________________________\n",
       "ReLU                 16 x 64 x 128 x 128  0          False     \n",
       "________________________________________________________________\n",
       "Conv2d               16 x 3 x 128 x 128   1,728      True      \n",
       "________________________________________________________________\n",
       "Tanh                 16 x 3 x 128 x 128   0          False     \n",
       "________________________________________________________________\n",
       "\n",
       "Total params: 3,216,960\n",
       "Total trainable params: 3,216,960\n",
       "Total non-trainable params: 0\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder2.change_out_size(128)\n",
    "enc_summary = torch.randn(16, 100)\n",
    "out = decoder2(enc_summary)\n",
    "test_eq(out.shape, (16, 3, 128, 128))\n",
    "decoder2.summary(enc_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MGenerator(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, inp_ids):\n",
    "        summary = self.encoder(inp_ids)\n",
    "        out = self.decoder(summary)\n",
    "        return out\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path: Path, vocab_path: Path,\n",
    "                       enc_out_size, dec_out_size, num_dec_layers):\n",
    "        vocab = json.loads(vocab_path.read_text())\n",
    "        encoder = Encoder(len(vocab), enc_out_size)\n",
    "        decoder = Decoder(dec_out_size, enc_out_size, num_dec_layers)\n",
    "        ret = cls(encoder, decoder)\n",
    "        ret.load_state_dict(torch.load(model_path))\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = MGenerator(encoder, decoder)\n",
    "inp_ids = torch.randint(0, 100, (16, 20))\n",
    "out = generator(inp_ids)\n",
    "test_eq(out.shape, (16, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_eda[script].ipynb.\n",
      "Converted 01_gen_coco_tiny_data[script].ipynb.\n",
      "Converted 02_data_coco.ipynb.\n",
      "Converted 03_model.ipynb.\n",
      "Converted 04_loss.ipynb.\n",
      "Converted 05_leaner.ipynb.\n",
      "Converted 90a_fulltest_train_lm.ipynb.\n",
      "Converted 90b_fulltest_train_generator.ipynb.\n",
      "Converted 95a_train_lm[script].ipynb.\n",
      "Converted 95b_train_generator[script].ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
