# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_model.ipynb (unless otherwise specified).

__all__ = ['Encoder', 'Decoder', 'MGenerator']

# Cell
import json
from fastai2.basics import *
from fastai2.text.all import *
from fastai2.vision.gan import *

# Cell
class Encoder(nn.Module):
    def __init__(self, vocab_size: int, out_size: int):
        super().__init__()
        self.awd_lstm = AWD_LSTM(
            vocab_size,
            emb_sz=400, n_hid=1152, n_layers=3, pad_token=1, bidir=False, hidden_p=0.15, input_p=0.25, embed_p=0.02, weight_p=0.2,
        )
        self.hid_proj = nn.Linear(self.awd_lstm.emb_sz, out_size)

    def forward(self, inp_ids):
        "inp_ids: (bs, seq_len), summary: (bs, out_size))"
        self.reset()
        awd_lstm_out = self.awd_lstm(inp_ids) # awd_lstm_out: (bs, seq_len, emb_sz=400)
        awd_lstm_hid = self.awd_lstm.hidden
        # hid[0], hid[1]: ( (1, bs, 1152), (1, bs, 1152) )
        # hid[2]: ( (1, bs, 400), (1, bs, 400) )

#         summary = awd_lstm_hid[2][0].squeeze(dim=0) # (bs, 400)
        summary = awd_lstm_out.mean(dim=1) # (bs, 400)
        summary = self.hid_proj(summary) # (bs, out_size)

        return summary
    def reset(self):
        self.awd_lstm.reset()
    @classmethod
    def from_pretrained(cls, awd_lstm_path: Path, vocab_path: Path,
                        out_size):
        vocab = json.loads(vocab_path.read_text())
        ret = cls(len(vocab), out_size)
        ret.awd_lstm.load_state_dict(torch.load(awd_lstm_path))
        return ret

# Cell
class Decoder(nn.Module):
    def __init__(self, out_size: int, inp_size: int, num_layers=3):
        super().__init__()
        store_attr(self, 'out_size,inp_size,num_layers')
        layers = self.gen_layers()
        self.layers = nn.Sequential(*layers)
    def gen_layers(self):
        up_sample = nn.Upsample(self.out_size//2**self.num_layers)
        num_c = 64*2**self.num_layers
        first_conv = ConvLayer(self.inp_size, num_c, 3, 1)
        other_conv = [ConvLayer(num_c//2**i, num_c//2**(i+1), 4, 2, 1, transpose=True) for i in range(self.num_layers)]
        last_conv = nn.Conv2d(64, 3, 3, 1, 1, bias=False)
        last_act = nn.Tanh()
        return [AddChannels(2), up_sample, first_conv] + other_conv + [last_conv, last_act]
    def change_out_size(self, out_size: int):
        self.out_size = out_size
        self.layers[1] = nn.Upsample(out_size//2**self.num_layers)
    def forward(self, enc_summary):
        "enc_summary: (bs, inp_size), out: (bs, 3, out_size, out_size)"
        return self.layers(enc_summary)

# Cell
class MGenerator(nn.Module):
    def __init__(self, encoder: Encoder, decoder: Decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
    def forward(self, inp_ids):
        summary = self.encoder(inp_ids)
        out = self.decoder(summary)
        return out
    @classmethod
    def from_pretrained(cls, model_path: Path, vocab_path: Path,
                       enc_out_size, dec_out_size, num_dec_layers):
        vocab = json.loads(vocab_path.read_text())
        encoder = Encoder(len(vocab), enc_out_size)
        decoder = Decoder(dec_out_size, enc_out_size, num_dec_layers)
        ret = cls(encoder, decoder)
        ret.load_state_dict(torch.load(model_path))
        return ret